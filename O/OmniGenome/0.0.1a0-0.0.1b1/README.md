# Comparing `tmp/OmniGenome-0.0.1a0-py3-none-any.whl.zip` & `tmp/OmniGenome-0.0.1b1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,99 +1,55 @@
-Zip file size: 98256 bytes, number of entries: 97
--rw-rw-rw-  2.0 fat     4823 b- defN 24-Apr-14 15:50 omnigenome/__init__.py
--rw-rw-rw-  2.0 fat        0 b- defN 24-Mar-16 16:29 omnigenome/_src/__init__.py
--rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-06 13:21 omnigenome/_src/abc/__init__.py
--rw-rw-rw-  2.0 fat    10028 b- defN 24-Apr-14 13:16 omnigenome/_src/abc/abstract_dataset.py
--rw-rw-rw-  2.0 fat     1773 b- defN 24-Apr-13 10:46 omnigenome/_src/abc/abstract_metric.py
--rw-rw-rw-  2.0 fat    10266 b- defN 24-Apr-14 13:16 omnigenome/_src/abc/abstract_model.py
--rw-rw-rw-  2.0 fat     1884 b- defN 24-Apr-14 13:16 omnigenome/_src/abc/abstract_tokenizer.py
--rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-06 13:42 omnigenome/_src/config/__init__.py
--rw-rw-rw-  2.0 fat     7654 b- defN 24-Apr-09 18:34 omnigenome/_src/config/config.py
--rw-rw-rw-  2.0 fat      971 b- defN 24-Apr-06 13:44 omnigenome/_src/config/config_check.py
--rw-rw-rw-  2.0 fat      689 b- defN 24-Apr-14 13:16 omnigenome/_src/dataset/__init__.py
--rw-rw-rw-  2.0 fat     8801 b- defN 24-Apr-14 10:40 omnigenome/_src/dataset/omnigenome_dataset.py
--rw-rw-rw-  2.0 fat      493 b- defN 24-Apr-14 10:52 omnigenome/_src/metric/__init__.py
--rw-rw-rw-  2.0 fat     2658 b- defN 24-Apr-12 21:15 omnigenome/_src/metric/classification_metric.py
--rw-rw-rw-  2.0 fat     2248 b- defN 24-Apr-12 21:15 omnigenome/_src/metric/ranking_metric.py
--rw-rw-rw-  2.0 fat     2626 b- defN 24-Apr-12 21:15 omnigenome/_src/metric/regression_metric.py
--rw-rw-rw-  2.0 fat        0 b- defN 24-Mar-16 16:34 omnigenome/_src/misc/__init__.py
--rw-rw-rw-  2.0 fat     5497 b- defN 24-Apr-14 13:16 omnigenome/_src/misc/utils.py
--rw-rw-rw-  2.0 fat      444 b- defN 24-Apr-14 10:45 omnigenome/_src/model/__init__.py
--rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-08 20:27 omnigenome/_src/model/classiifcation/__init__.py
--rw-rw-rw-  2.0 fat    18734 b- defN 24-Apr-14 13:16 omnigenome/_src/model/classiifcation/model.py
--rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-10 12:33 omnigenome/_src/model/mlm/__init__.py
--rw-rw-rw-  2.0 fat     4136 b- defN 24-Apr-14 12:29 omnigenome/_src/model/mlm/model.py
--rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-08 20:27 omnigenome/_src/model/regression/__init__.py
--rw-rw-rw-  2.0 fat    18876 b- defN 24-Apr-14 12:29 omnigenome/_src/model/regression/model.py
--rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-08 21:23 omnigenome/_src/model/seq2seq/__init__.py
--rw-rw-rw-  2.0 fat      339 b- defN 24-Apr-14 10:41 omnigenome/_src/model/seq2seq/model.py
--rw-rw-rw-  2.0 fat      512 b- defN 24-Apr-14 10:45 omnigenome/_src/tokenizer/__init__.py
--rw-rw-rw-  2.0 fat     2534 b- defN 24-Apr-14 10:41 omnigenome/_src/tokenizer/bpe_tokenizer.py
--rw-rw-rw-  2.0 fat     3857 b- defN 24-Apr-14 10:42 omnigenome/_src/tokenizer/kmers_tokenizer.py
--rw-rw-rw-  2.0 fat     3786 b- defN 24-Apr-14 10:43 omnigenome/_src/tokenizer/single_nucleotide_tokenizer.py
--rw-rw-rw-  2.0 fat      409 b- defN 24-Apr-14 10:45 omnigenome/_src/trainer/__init__.py
--rw-rw-rw-  2.0 fat     1092 b- defN 24-Apr-09 16:39 omnigenome/_src/trainer/hf_trainer.py
--rw-rw-rw-  2.0 fat     5958 b- defN 24-Apr-14 13:16 omnigenome/_src/trainer/trainer.py
+Zip file size: 50829 bytes, number of entries: 53
+-rw-rw-rw-  2.0 fat     4822 b- defN 24-Apr-18 19:07 omnigenome/__init__.py
 -rw-rw-rw-  2.0 fat        0 b- defN 24-Mar-16 16:24 omnigenome/bench/__init__.py
 -rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-11 17:29 omnigenome/bench/auto_bench/__init__.py
 -rw-rw-rw-  2.0 fat      472 b- defN 24-Apr-14 10:55 omnigenome/bench/auto_bench/auto_bench.py
 -rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-11 17:29 omnigenome/bench/bench_hub/__init__.py
 -rw-rw-rw-  2.0 fat      439 b- defN 24-Apr-14 13:16 omnigenome/bench/bench_hub/bench_hub.py
 -rw-rw-rw-  2.0 fat        0 b- defN 24-Mar-16 16:29 omnigenome/src/__init__.py
 -rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-06 13:21 omnigenome/src/abc/__init__.py
--rw-rw-rw-  2.0 fat    10028 b- defN 24-Apr-14 13:16 omnigenome/src/abc/abstract_dataset.py
+-rw-rw-rw-  2.0 fat    10223 b- defN 24-Apr-14 23:58 omnigenome/src/abc/abstract_dataset.py
 -rw-rw-rw-  2.0 fat     1773 b- defN 24-Apr-13 10:46 omnigenome/src/abc/abstract_metric.py
--rw-rw-rw-  2.0 fat    10238 b- defN 24-Apr-14 15:39 omnigenome/src/abc/abstract_model.py
--rw-rw-rw-  2.0 fat     1884 b- defN 24-Apr-14 13:16 omnigenome/src/abc/abstract_tokenizer.py
+-rw-rw-rw-  2.0 fat    10338 b- defN 24-Apr-21 09:30 omnigenome/src/abc/abstract_model.py
+-rw-rw-rw-  2.0 fat     2501 b- defN 24-Apr-21 09:30 omnigenome/src/abc/abstract_tokenizer.py
 -rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-06 13:42 omnigenome/src/config/__init__.py
 -rw-rw-rw-  2.0 fat     7654 b- defN 24-Apr-09 18:34 omnigenome/src/config/config.py
 -rw-rw-rw-  2.0 fat      971 b- defN 24-Apr-06 13:44 omnigenome/src/config/config_check.py
 -rw-rw-rw-  2.0 fat      689 b- defN 24-Apr-14 13:16 omnigenome/src/dataset/__init__.py
--rw-rw-rw-  2.0 fat     8801 b- defN 24-Apr-14 10:40 omnigenome/src/dataset/omnigenome_dataset.py
+-rw-rw-rw-  2.0 fat     8737 b- defN 24-Apr-15 13:59 omnigenome/src/dataset/omnigenome_dataset.py
 -rw-rw-rw-  2.0 fat      493 b- defN 24-Apr-14 10:52 omnigenome/src/metric/__init__.py
 -rw-rw-rw-  2.0 fat     2658 b- defN 24-Apr-12 21:15 omnigenome/src/metric/classification_metric.py
 -rw-rw-rw-  2.0 fat     2248 b- defN 24-Apr-12 21:15 omnigenome/src/metric/ranking_metric.py
 -rw-rw-rw-  2.0 fat     2626 b- defN 24-Apr-12 21:15 omnigenome/src/metric/regression_metric.py
 -rw-rw-rw-  2.0 fat        0 b- defN 24-Mar-16 16:34 omnigenome/src/misc/__init__.py
--rw-rw-rw-  2.0 fat     5497 b- defN 24-Apr-14 13:16 omnigenome/src/misc/utils.py
+-rw-rw-rw-  2.0 fat     5586 b- defN 24-Apr-21 09:30 omnigenome/src/misc/utils.py
 -rw-rw-rw-  2.0 fat      470 b- defN 24-Apr-14 14:46 omnigenome/src/model/__init__.py
 -rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-08 20:27 omnigenome/src/model/classiifcation/__init__.py
--rw-rw-rw-  2.0 fat    18734 b- defN 24-Apr-14 13:16 omnigenome/src/model/classiifcation/model.py
+-rw-rw-rw-  2.0 fat    18174 b- defN 24-Apr-16 13:56 omnigenome/src/model/classiifcation/model.py
 -rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-10 12:33 omnigenome/src/model/mlm/__init__.py
--rw-rw-rw-  2.0 fat     4136 b- defN 24-Apr-14 12:29 omnigenome/src/model/mlm/model.py
+-rw-rw-rw-  2.0 fat     4434 b- defN 24-Apr-21 09:30 omnigenome/src/model/mlm/model.py
 -rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-08 20:27 omnigenome/src/model/regression/__init__.py
--rw-rw-rw-  2.0 fat    18876 b- defN 24-Apr-14 12:29 omnigenome/src/model/regression/model.py
+-rw-rw-rw-  2.0 fat    18316 b- defN 24-Apr-16 11:30 omnigenome/src/model/regression/model.py
 -rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-08 21:23 omnigenome/src/model/seq2seq/__init__.py
 -rw-rw-rw-  2.0 fat      601 b- defN 24-Apr-14 14:48 omnigenome/src/model/seq2seq/model.py
 -rw-rw-rw-  2.0 fat      512 b- defN 24-Apr-14 10:45 omnigenome/src/tokenizer/__init__.py
--rw-rw-rw-  2.0 fat     2534 b- defN 24-Apr-14 10:41 omnigenome/src/tokenizer/bpe_tokenizer.py
--rw-rw-rw-  2.0 fat     3857 b- defN 24-Apr-14 10:42 omnigenome/src/tokenizer/kmers_tokenizer.py
--rw-rw-rw-  2.0 fat     3786 b- defN 24-Apr-14 10:43 omnigenome/src/tokenizer/single_nucleotide_tokenizer.py
+-rw-rw-rw-  2.0 fat     2799 b- defN 24-Apr-15 13:21 omnigenome/src/tokenizer/bpe_tokenizer.py
+-rw-rw-rw-  2.0 fat     4312 b- defN 24-Apr-21 09:30 omnigenome/src/tokenizer/kmers_tokenizer.py
+-rw-rw-rw-  2.0 fat     3132 b- defN 24-Apr-21 09:30 omnigenome/src/tokenizer/single_nucleotide_tokenizer.py
 -rw-rw-rw-  2.0 fat      409 b- defN 24-Apr-14 10:45 omnigenome/src/trainer/__init__.py
 -rw-rw-rw-  2.0 fat     1092 b- defN 24-Apr-09 16:39 omnigenome/src/trainer/hf_trainer.py
--rw-rw-rw-  2.0 fat     5400 b- defN 24-Apr-14 13:25 omnigenome/src/trainer/trainer.py
+-rw-rw-rw-  2.0 fat     5857 b- defN 24-Apr-19 19:11 omnigenome/src/trainer/trainer.py
 -rw-rw-rw-  2.0 fat        0 b- defN 24-Mar-16 16:25 omnigenome/utility/__init__.py
 -rw-rw-rw-  2.0 fat    10682 b- defN 24-Apr-14 14:32 omnigenome/utility/hub_utils.py
 -rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-11 17:29 omnigenome/utility/model_hub/__init__.py
--rw-rw-rw-  2.0 fat     3689 b- defN 24-Apr-14 15:39 omnigenome/utility/model_hub/model_hub.py
+-rw-rw-rw-  2.0 fat     3641 b- defN 24-Apr-15 16:14 omnigenome/utility/model_hub/model_hub.py
 -rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-06 13:21 omnigenome/utility/pipeline_hub/__init__.py
--rw-rw-rw-  2.0 fat     6155 b- defN 24-Apr-14 15:42 omnigenome/utility/pipeline_hub/pipeline.py
+-rw-rw-rw-  2.0 fat     5744 b- defN 24-Apr-16 14:05 omnigenome/utility/pipeline_hub/pipeline.py
 -rw-rw-rw-  2.0 fat      883 b- defN 24-Apr-14 15:43 omnigenome/utility/pipeline_hub/pipeline_hub.py
 -rw-rw-rw-  2.0 fat      342 b- defN 24-Jan-20 14:56 tutorials/__init__.py
 -rw-rw-rw-  2.0 fat     1660 b- defN 24-Feb-11 02:51 tutorials/codonbert_adapter.py
--rw-rw-rw-  2.0 fat     4710 b- defN 24-Apr-14 14:32 tutorials/mRNA_test.py
--rw-rw-rw-  2.0 fat     3463 b- defN 24-Apr-14 14:32 tutorials/mlm_service_test.py
--rw-rw-rw-  2.0 fat     3496 b- defN 24-Apr-14 14:32 tutorials/pc_test.py
--rw-rw-rw-  2.0 fat     3400 b- defN 24-Apr-14 14:32 tutorials/psp_test.py
--rw-rw-rw-  2.0 fat     4408 b- defN 24-Apr-14 14:32 tutorials/snmd_test.py
--rw-rw-rw-  2.0 fat     4610 b- defN 24-Apr-14 14:32 tutorials/snmr_test.py
--rw-rw-rw-  2.0 fat     3471 b- defN 24-Apr-14 14:32 tutorials/snp_test.py
--rw-rw-rw-  2.0 fat     4049 b- defN 24-Apr-14 14:32 tutorials/ssp_test.py
 -rw-rw-rw-  2.0 fat     8409 b- defN 24-Apr-09 16:39 tutorials/ssp_validation.py
--rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-14 02:15 tutorials/utility/__init__.py
--rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-14 02:15 tutorials/utility/pipeline_examples/__init__.py
--rw-rw-rw-  2.0 fat     1408 b- defN 24-Apr-14 13:16 tutorials/utility/pipeline_examples/ssp_pipeline.py
--rw-rw-rw-  2.0 fat      835 b- defN 24-Apr-14 15:50 OmniGenome-0.0.1a0.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 24-Apr-14 15:50 OmniGenome-0.0.1a0.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       21 b- defN 24-Apr-14 15:50 OmniGenome-0.0.1a0.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     8941 b- defN 24-Apr-14 15:50 OmniGenome-0.0.1a0.dist-info/RECORD
-97 files, 318846 bytes uncompressed, 83830 bytes compressed:  73.7%
+-rw-rw-rw-  2.0 fat      861 b- defN 24-Apr-21 09:37 OmniGenome-0.0.1b1.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 24-Apr-21 09:37 OmniGenome-0.0.1b1.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       21 b- defN 24-Apr-21 09:37 OmniGenome-0.0.1b1.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat     4877 b- defN 24-Apr-21 09:37 OmniGenome-0.0.1b1.dist-info/RECORD
+53 files, 162970 bytes uncompressed, 42893 bytes compressed:  73.7%
```

## zipnote {}

```diff
@@ -1,109 +1,10 @@
 Filename: omnigenome/__init__.py
 Comment: 
 
-Filename: omnigenome/_src/__init__.py
-Comment: 
-
-Filename: omnigenome/_src/abc/__init__.py
-Comment: 
-
-Filename: omnigenome/_src/abc/abstract_dataset.py
-Comment: 
-
-Filename: omnigenome/_src/abc/abstract_metric.py
-Comment: 
-
-Filename: omnigenome/_src/abc/abstract_model.py
-Comment: 
-
-Filename: omnigenome/_src/abc/abstract_tokenizer.py
-Comment: 
-
-Filename: omnigenome/_src/config/__init__.py
-Comment: 
-
-Filename: omnigenome/_src/config/config.py
-Comment: 
-
-Filename: omnigenome/_src/config/config_check.py
-Comment: 
-
-Filename: omnigenome/_src/dataset/__init__.py
-Comment: 
-
-Filename: omnigenome/_src/dataset/omnigenome_dataset.py
-Comment: 
-
-Filename: omnigenome/_src/metric/__init__.py
-Comment: 
-
-Filename: omnigenome/_src/metric/classification_metric.py
-Comment: 
-
-Filename: omnigenome/_src/metric/ranking_metric.py
-Comment: 
-
-Filename: omnigenome/_src/metric/regression_metric.py
-Comment: 
-
-Filename: omnigenome/_src/misc/__init__.py
-Comment: 
-
-Filename: omnigenome/_src/misc/utils.py
-Comment: 
-
-Filename: omnigenome/_src/model/__init__.py
-Comment: 
-
-Filename: omnigenome/_src/model/classiifcation/__init__.py
-Comment: 
-
-Filename: omnigenome/_src/model/classiifcation/model.py
-Comment: 
-
-Filename: omnigenome/_src/model/mlm/__init__.py
-Comment: 
-
-Filename: omnigenome/_src/model/mlm/model.py
-Comment: 
-
-Filename: omnigenome/_src/model/regression/__init__.py
-Comment: 
-
-Filename: omnigenome/_src/model/regression/model.py
-Comment: 
-
-Filename: omnigenome/_src/model/seq2seq/__init__.py
-Comment: 
-
-Filename: omnigenome/_src/model/seq2seq/model.py
-Comment: 
-
-Filename: omnigenome/_src/tokenizer/__init__.py
-Comment: 
-
-Filename: omnigenome/_src/tokenizer/bpe_tokenizer.py
-Comment: 
-
-Filename: omnigenome/_src/tokenizer/kmers_tokenizer.py
-Comment: 
-
-Filename: omnigenome/_src/tokenizer/single_nucleotide_tokenizer.py
-Comment: 
-
-Filename: omnigenome/_src/trainer/__init__.py
-Comment: 
-
-Filename: omnigenome/_src/trainer/hf_trainer.py
-Comment: 
-
-Filename: omnigenome/_src/trainer/trainer.py
-Comment: 
-
 Filename: omnigenome/bench/__init__.py
 Comment: 
 
 Filename: omnigenome/bench/auto_bench/__init__.py
 Comment: 
 
 Filename: omnigenome/bench/auto_bench/auto_bench.py
@@ -237,56 +138,23 @@
 
 Filename: tutorials/__init__.py
 Comment: 
 
 Filename: tutorials/codonbert_adapter.py
 Comment: 
 
-Filename: tutorials/mRNA_test.py
-Comment: 
-
-Filename: tutorials/mlm_service_test.py
-Comment: 
-
-Filename: tutorials/pc_test.py
-Comment: 
-
-Filename: tutorials/psp_test.py
-Comment: 
-
-Filename: tutorials/snmd_test.py
-Comment: 
-
-Filename: tutorials/snmr_test.py
-Comment: 
-
-Filename: tutorials/snp_test.py
-Comment: 
-
-Filename: tutorials/ssp_test.py
-Comment: 
-
 Filename: tutorials/ssp_validation.py
 Comment: 
 
-Filename: tutorials/utility/__init__.py
-Comment: 
-
-Filename: tutorials/utility/pipeline_examples/__init__.py
-Comment: 
-
-Filename: tutorials/utility/pipeline_examples/ssp_pipeline.py
-Comment: 
-
-Filename: OmniGenome-0.0.1a0.dist-info/METADATA
+Filename: OmniGenome-0.0.1b1.dist-info/METADATA
 Comment: 
 
-Filename: OmniGenome-0.0.1a0.dist-info/WHEEL
+Filename: OmniGenome-0.0.1b1.dist-info/WHEEL
 Comment: 
 
-Filename: OmniGenome-0.0.1a0.dist-info/top_level.txt
+Filename: OmniGenome-0.0.1b1.dist-info/top_level.txt
 Comment: 
 
-Filename: OmniGenome-0.0.1a0.dist-info/RECORD
+Filename: OmniGenome-0.0.1b1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## omnigenome/__init__.py

```diff
@@ -4,15 +4,15 @@
 # author: YANG, HENG <hy345@exeter.ac.uk> (杨恒)
 # github: https://github.com/yangheng95
 # huggingface: https://huggingface.co/yangheng
 # google scholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
 # Copyright (C) 2019-2024. All Rights Reserved.
 
 __name__ = "OmniGenome"
-__version__ = "0.0.1alpha0"
+__version__ = "0.0.1beta1"
 __author__ = "YANG, HENG"
 __email__ = "yangheng2021@gmail.com"
 __license__ = "MIT"
 
 
 from .src.abc.abstract_dataset import OmniGenomeDataset
 from .src.abc.abstract_model import OmniGenomeModel
```

## omnigenome/src/abc/abstract_dataset.py

```diff
@@ -72,14 +72,15 @@
             fprint(
                 f"Detected max_length={self.tokenizer.max_length} from the tokenizer."
             )
             self.max_length = self.tokenizer.max_length
         else:
             raise ValueError("max_length must be provided in the dataset or tokenizer.")
 
+        self.tokenizer.max_length = self.max_length
         self.examples = []
         self.data = []
 
         if os.path.exists(data_source):
             fprint(f"Loading data from {data_source}...")
             self.load_data_source(data_source, **kwargs)
         else:
@@ -128,15 +129,17 @@
                         elif key == "attention_mask":
                             _pad_value = torch.zeros((padding_length, value.size(1)))
                         elif "label" in key or "labels" in key:
                             _pad_value = -100 * torch.ones(
                                 (padding_length, value.size(1))
                             )
                         else:
-                            _pad_value = pad_value
+                            _pad_value = pad_value * torch.ones(
+                                (padding_length, value.size(1))
+                            )
                         data_item[key] = torch.cat([value, _pad_value], dim=0)
                     elif padding_length < 0:
                         data_item[key] = value[:max_length]
                     data_item[key] = data_item[key].to(dtype)
 
                 elif isinstance(value, torch.Tensor) and value.dim() == 1:
                     padding_length = max_length - value.size(0)
@@ -152,15 +155,15 @@
                                     * torch.ones((padding_length,))
                                 )
                         elif key == "attention_mask":
                             _pad_value = torch.zeros((padding_length,))
                         elif "label" in key or "labels" in key:
                             _pad_value = -100 * torch.ones((padding_length,))
                         else:
-                            _pad_value = pad_value
+                            _pad_value = pad_value * torch.ones((padding_length,))
                         data_item[key] = torch.cat([value, _pad_value], dim=0)
                     elif padding_length < 0:
                         data_item[key] = value[:max_length]
 
                     data_item[key] = data_item[key].to(dtype)
 
     def load_data_source(self, data_source, **kwargs):
```

## omnigenome/src/abc/abstract_model.py

```diff
@@ -7,15 +7,15 @@
 # google scholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
 # Copyright (C) 2019-2024. All Rights Reserved.
 import json
 import os
 import warnings
 
 import torch
-from ViennaRNA import RNA
+
 from transformers import AutoModel, AutoConfig, AutoTokenizer
 
 from ..misc.utils import fprint, env_meta_info
 
 
 from ..misc.utils import RNA2StructureCache
 
@@ -65,16 +65,18 @@
 
     assert (
         "last_hidden_state" in outputs
     ), f"last_hidden_state not found in the outputs from the {model.__class__.__name__}"
     last_hidden_state = outputs["last_hidden_state"]
 
     if ss == "viennarna":
-        sequences = tokenizer.batch_decode(input_ids, skip_special_tokens=True)
-        structures = [rna2structure.fold(seq)[0] for seq in sequences]
+        sequences = tokenizer.base_tokenizer.batch_decode(
+            input_ids, skip_special_tokens=True
+        )
+        structures = [rna2structure.fold(seq.replace(" ", ""))[0] for seq in sequences]
         tokenized_struct = tokenizer(
             structures,
             padding="max_length",
             max_length=input_ids.shape[1],
             truncation=True,
             return_tensors="pt",
             add_special_tokens=False,
@@ -136,15 +138,15 @@
         self.dropout = torch.nn.Dropout(kwargs.get("dropout", 0.0))
         self.activation = torch.nn.Tanh()
 
         for key, value in kwargs.items():
             self.metadata[key] = value
 
         fprint(
-            f"The trainable parameters of the model are: {count_parameters(self.model) / 1e6:.2f} Millions"
+            f"The trainable parameters of the {self.model.__class__.__name__} model are: {count_parameters(self.model) / 1e6:.2f} Millions"
         )
 
     def loss_function(self, logits, labels):
         raise NotImplementedError(
             "The loss_function() function should be implemented for your model."
         )
 
@@ -200,14 +202,15 @@
         else:
             raise RuntimeError(
                 "The output of the forward() function should be a dictionary-like objective"
                 " and have either 'loss', or 'logits' and 'labels' attribute."
             )
 
     def save(self, path, overwrite=False, **kwargs):
+        self.eval()
         import dill
 
         if os.path.exists(path) and not overwrite:
             raise FileExistsError(
                 f"The path {path} already exists, please set overwrite=True to overwrite it."
             )
 
@@ -217,44 +220,44 @@
         device = self.model.device
 
         self.model.to("cpu")
         with open(f"{path}/tokenizer.pkl", "wb") as f:
             dill.dump(self.tokenizer, f)
         with open(f"{path}/metadata.json", "w", encoding="utf8") as f:
             json.dump(self.metadata, f)
-        self.model.save_pretrained(path, safe_serialization=False)
-        self.config.save_pretrained(path)
+        self.model.save_pretrained(
+            f"{path}", safe_serialization=False
+        )  # do not remove this line, used to save customed model scripts
+        with open(f"{path}/pytorch_model.bin", "wb") as f:
+            torch.save(self.state_dict(), f)
         self.model.to(device)
+        fprint(f"The model is saved to {path}.")
 
     def load(self, path, **kwargs):
         with open(f"{path}/metadata.json", "r", encoding="utf8") as f:
             metadata = json.load(f)
 
         if metadata["model_cls"] != self.__class__.__name__:  # Check the model class
             raise ValueError(
                 f"The model class in the loaded model is {metadata['model_cls']}, "
                 f"but the current model class is {self.__class__.__name__}."
             )
         config = AutoConfig.from_pretrained(path, trust_remote_code=True, **kwargs)
-        self.config.from_pretrained(path, trust_remote_code=True, **kwargs)
-        with open(f"{path}/config.json", "r", encoding="utf8") as f:
-            config.__dict__ = json.load(f)
 
         for key, value in config.__dict__.items():
             if key not in self.config.__dict__ or self.config.__dict__[key] != value:
                 fprint(
                     f"Warning: The value of the key {key} in the loaded model is {value}, "
                     f"but the current value is {self.config.__dict__.get(key, None)}."
                 )
 
         with open(f"{path}/pytorch_model.bin", "rb") as f:
-            self.model.load_state_dict(
-                torch.load(f, map_location=self.model.device), strict=False
+            self.load_state_dict(
+                torch.load(f, map_location=kwargs.get("device", "cpu")), strict=True
             )
-
         return self
 
     def load_tokenizer(self, path):
         import dill
 
         with open(f"{path}/tokenizer.pkl", "rb") as f:
             tokenizer = dill.load(f)
```

## omnigenome/src/abc/abstract_tokenizer.py

```diff
@@ -20,26 +20,43 @@
 
         self.base_tokenizer = base_tokenizer
         self.max_length = max_length
 
         for key, value in kwargs.items():
             self.metadata[key] = value
 
+        self.u2t = kwargs.get("u2t", False)
+        self.t2u = kwargs.get("t2u", False)
+        self.add_whitespace = kwargs.get("add_whitespace", False)
+
     @staticmethod
     def from_pretrained(model_name_or_path, **kwargs):
         self = OmniGenomeTokenizer(
             AutoTokenizer.from_pretrained(model_name_or_path, **kwargs)
         )
         return self
 
     def save_pretrained(self, save_directory):
         self.base_tokenizer.save_pretrained(save_directory)
 
     def __call__(self, *args, **kwargs):
-        return self.base_tokenizer(*args, **kwargs)
+        padding = kwargs.pop("padding", True)
+        truncation = kwargs.pop("truncation", True)
+        max_length = kwargs.pop(
+            "max_length", self.max_length if self.max_length else 512
+        )
+        return_tensor = kwargs.pop("return_tensors", "pt")
+        return self.base_tokenizer(
+            padding=padding,
+            truncation=truncation,
+            max_length=max_length,
+            return_tensors=return_tensor,
+            *args,
+            **kwargs
+        )
 
     def tokenize(self, sequence, **kwargs):
         raise NotImplementedError(
             "The tokenize() function should be adapted for different models,"
             " please implement it for your model."
         )
```

## omnigenome/src/dataset/omnigenome_dataset.py

```diff
@@ -53,15 +53,15 @@
                 raise Exception(
                     "The input instance must contain a 'seq' or 'sequence' key."
                 )
         else:
             raise Exception("Unknown instance format.")
 
         tokenized_inputs = self.tokenizer(
-            " ".join(list(sequence)),
+            sequence,
             padding="do_not_pad",
             truncation=True,
             max_length=self.max_length,
             return_tensors="pt",
             **kwargs
         )
         for col in tokenized_inputs:
@@ -103,15 +103,15 @@
             label = instance.get("label", None)
             labels = instance.get("labels", None)
             labels = labels if labels is not None else label
         else:
             raise Exception("Unknown instance format.")
 
         tokenized_inputs = self.tokenizer(
-            " ".join(list(sequence)),
+            sequence,
             padding="do_not_pad",
             truncation=True,
             max_length=self.max_length,
             return_tensors="pt",
             **kwargs
         )
         for col in tokenized_inputs:
@@ -149,15 +149,15 @@
             label = instance.get("label", None)
             labels = instance.get("labels", None)
             labels = labels if labels is not None else label
         else:
             raise Exception("Unknown instance format.")
 
         tokenized_inputs = self.tokenizer(
-            " ".join(list(sequence)),
+            sequence,
             padding="do_not_pad",
             truncation=True,
             max_length=self.max_length,
             return_tensors="pt",
             **kwargs
         )
         for col in tokenized_inputs:
@@ -218,15 +218,15 @@
             label = instance.get("label", None)
             labels = instance.get("labels", None)
             labels = labels if labels is not None else label
         else:
             raise Exception("Unknown instance format.")
 
         tokenized_inputs = self.tokenizer(
-            " ".join(list(sequence)),
+            sequence,
             padding="do_not_pad",
             truncation=True,
             max_length=self.max_length,
             return_tensors="pt",
             **kwargs
         )
         for col in tokenized_inputs:
```

## omnigenome/src/misc/utils.py

```diff
@@ -29,26 +29,23 @@
     def __init__(self, cache_file=None, *args, **kwargs):
         import RNA
 
         self.RNA = RNA
         super().__init__(*args, **kwargs)
 
         if not cache_file:
-            cache_file = "__OMNIGENOME_DATA__/rna2stucture.cache.pkl"
-
-        self.cache_file = cache_file
-
-        if not os.path.exists(os.path.dirname(cache_file)):
-            os.makedirs(os.path.dirname(cache_file))
+            self.cache_file = "__OMNIGENOME_DATA__/rna2stucture.cache.pkl"
+        else:
+            self.cache_file = cache_file
 
-        if not os.path.exists(cache_file):
+        if self.cache_file is None or not os.path.exists(self.cache_file):
             self.cache = {}
         else:
-            print(f"Loading cache from {cache_file}...")
-            with open(cache_file, "rb") as f:
+            print(f"Loading cache from {self.cache_file}...")
+            with open(self.cache_file, "rb") as f:
                 self.cache = pickle.load(f)
 
         self.queue_num = 0
 
     def __getitem__(self, key):
         return self.cache[key]
 
@@ -73,18 +70,23 @@
     def update_cache_file(self, cache_file=None):
         if self.queue_num < 100:
             return
 
         if cache_file is None:
             cache_file = self.cache_file
 
+        if not os.path.exists(os.path.dirname(cache_file)):
+            os.makedirs(os.path.dirname(cache_file))
+
         print(f"Updating cache file {cache_file}...")
-        with open(self.cache_file, "wb") as f:
+        with open(cache_file, "wb") as f:
             pickle.dump(self.cache, f)
 
+        self.queue_num = 0
+
 
 def env_meta_info():
     from torch.version import __version__ as torch_version
     from torch.version import cuda as torch_cuda_version
     from torch.version import git_version
     from transformers import __version__ as transformers_version
     from ... import __version__ as omnigenome_version
```

## omnigenome/src/model/classiifcation/model.py

```diff
@@ -4,14 +4,15 @@
 # author: YANG, HENG <hy345@exeter.ac.uk> (杨恒)
 # github: https://github.com/yangheng95
 # huggingface: https://huggingface.co/yangheng
 # google scholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
 # Copyright (C) 2019-2024. All Rights Reserved.
 
 import torch
+from transformers import BatchEncoding
 from transformers.models.bert.modeling_bert import BertPooler
 
 from ...abc.abstract_model import OmniGenomeModel
 from ...abc.abstract_model import extract_last_hidden_state
 
 
 class OmniGenomeEncoderModelForTokenClassification(OmniGenomeModel):
@@ -27,68 +28,59 @@
         last_hidden_state = self.activation(last_hidden_state)
         logits = self.classifier(last_hidden_state)
         logits = self.softmax(logits)
         outputs = {"logits": logits, "last_hidden_state": last_hidden_state}
         return outputs
 
     def predict(self, sequence_or_inputs, **kwargs):
-        if isinstance(sequence_or_inputs, str):
+        if not isinstance(sequence_or_inputs, BatchEncoding) and not isinstance(
+            sequence_or_inputs, dict
+        ):
             inputs = self.tokenizer(sequence_or_inputs, return_tensors="pt", **kwargs)
         else:
             inputs = sequence_or_inputs
+        inputs = inputs.to(self.model.device)
 
-        if len(inputs["input_ids"].shape) == 1:
-            for col in inputs:
-                inputs[col] = inputs[col].to(self.model.device).unsqueeze(0)
-        else:
-            for col in inputs:
-                inputs[col] = inputs[col].to(self.model.device)
-
-        outputs = self(inputs)
+        with torch.no_grad():
+            outputs = self(inputs)
         logits = outputs["logits"]
         last_hidden_state = outputs["last_hidden_state"]
 
         predictions = []
         for i in range(logits.shape[0]):
             predictions.append(logits[i].argmax(dim=-1).detach().cpu().numpy())
 
-        if len(predictions) == 1:
-            outputs = {
-                "predictions": predictions[0],
-                "logits": logits[0],
-                "last_hidden_state": last_hidden_state[0],
-            }
-        else:
-            outputs = {
-                "predictions": predictions,
-                "logits": logits,
-                "last_hidden_state": last_hidden_state,
-            }
+        outputs = {
+            "predictions": predictions,
+            "logits": logits,
+            "last_hidden_state": last_hidden_state,
+        }
 
         return outputs
 
     def inference(self, sequence_or_inputs, **kwargs):
         inputs = self.tokenizer(sequence_or_inputs, return_tensors="pt", **kwargs)
         inputs = inputs.to(self.model.device)
 
-        outputs = self(inputs)
-        logits = outputs["logits"]
-        last_hidden_state = outputs["last_hidden_state"]
+        with torch.no_grad():
+            outputs = self(inputs)
+        logits = outputs["logits"][:, 1:-1:, :]
+        last_hidden_state = outputs["last_hidden_state"][:, 1:-1:, :]
 
         predictions = []
         for i in range(logits.shape[0]):
             i_logit = logits[i][
-                : inputs["input_ids"][0].ne(self.config.pad_token_id).sum(dim=-1)
-            ][1:-1]
+                : inputs["input_ids"][i].ne(self.config.pad_token_id).sum(dim=-1)
+            ]
             prediction = [
                 self.config.id2label.get(x.item(), "") for x in i_logit.argmax(dim=-1)
             ]
             predictions.append(prediction)
 
-        if len(predictions) == 1:
+        if not isinstance(sequence_or_inputs, list):
             outputs = {
                 "predictions": predictions[0],
                 "logits": logits[0],
                 "last_hidden_state": last_hidden_state[0],
             }
         else:
             outputs = {
@@ -120,64 +112,55 @@
         pooled_output = self.pooler(last_hidden_state)
         logits = self.classifier(pooled_output)
         logits = self.softmax(logits)
         outputs = {"logits": logits, "last_hidden_state": last_hidden_state}
         return outputs
 
     def predict(self, sequence_or_inputs, **kwargs):
-        if isinstance(sequence_or_inputs, str):
+        if not isinstance(sequence_or_inputs, BatchEncoding) and not isinstance(
+            sequence_or_inputs, dict
+        ):
             inputs = self.tokenizer(sequence_or_inputs, return_tensors="pt", **kwargs)
         else:
             inputs = sequence_or_inputs
+        inputs = inputs.to(self.model.device)
 
-        if len(inputs["input_ids"].shape) == 1:
-            for col in inputs:
-                inputs[col] = inputs[col].to(self.model.device).unsqueeze(0)
-        else:
-            for col in inputs:
-                inputs[col] = inputs[col].to(self.model.device)
-
-        outputs = self(inputs)
+        with torch.no_grad():
+            outputs = self(inputs)
         logits = outputs["logits"]
         last_hidden_state = outputs["last_hidden_state"]
 
         predictions = []
         for i in range(logits.shape[0]):
             predictions.append(logits[i].argmax(dim=-1).item())
 
-        if len(predictions) == 1:
-            outputs = {
-                "predictions": predictions[0],
-                "logits": logits[0],
-                "last_hidden_state": last_hidden_state[0],
-            }
-        else:
-            outputs = {
-                "predictions": predictions,
-                "logits": logits,
-                "last_hidden_state": last_hidden_state,
-            }
+        outputs = {
+            "predictions": predictions,
+            "logits": logits,
+            "last_hidden_state": last_hidden_state,
+        }
 
         return outputs
 
     def inference(self, sequence_or_inputs, **kwargs):
         inputs = self.tokenizer(sequence_or_inputs, return_tensors="pt", **kwargs)
         inputs = inputs.to(self.model.device)
 
-        outputs = self(inputs)
+        with torch.no_grad():
+            outputs = self(inputs)
         logits = outputs["logits"]
         last_hidden_state = outputs["last_hidden_state"]
 
         predictions = []
         for i in range(logits.shape[0]):
             predictions.append(
                 self.config.id2label.get(logits[i].argmax(dim=-1).item(), "")
             )
 
-        if len(predictions) == 1:
+        if not isinstance(sequence_or_inputs, list):
             outputs = {
                 "predictions": predictions[0],
                 "logits": logits[0],
                 "last_hidden_state": last_hidden_state[0],
             }
         else:
             outputs = {
```

## omnigenome/src/model/mlm/model.py

```diff
@@ -4,14 +4,15 @@
 # author: YANG, HENG <hy345@exeter.ac.uk> (杨恒)
 # github: https://github.com/yangheng95
 # huggingface: https://huggingface.co/yangheng
 # google scholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
 # Copyright (C) 2019-2024. All Rights Reserved.
 
 import torch
+from transformers import BatchEncoding
 
 from ...abc.abstract_model import OmniGenomeModel
 
 
 class OmniGenomeEncoderModelForMLM(OmniGenomeModel):
     def __init__(self, config, base_model, tokenizer, *args, **kwargs):
         super().__init__(config, base_model, tokenizer, *args, **kwargs)
@@ -36,35 +37,32 @@
             "loss": loss,
             "logits": logits,
             "last_hidden_state": last_hidden_state,
         }
         return outputs
 
     def predict(self, sequence_or_inputs, **kwargs):
-        if isinstance(sequence_or_inputs, str):
+        if not isinstance(sequence_or_inputs, BatchEncoding) and not isinstance(
+            sequence_or_inputs, dict
+        ):
             inputs = self.tokenizer(sequence_or_inputs, return_tensors="pt", **kwargs)
         else:
             inputs = sequence_or_inputs
+        inputs = inputs.to(self.model.device)
 
-        if len(inputs["input_ids"].shape) == 1:
-            for col in inputs:
-                inputs[col] = inputs[col].to(self.model.device).unsqueeze(0)
-        else:
-            for col in inputs:
-                inputs[col] = inputs[col].to(self.model.device)
-
-        outputs = self(inputs)
+        with torch.no_grad():
+            outputs = self(inputs)
         logits = outputs["logits"]
         last_hidden_state = outputs["last_hidden_state"]
 
         predictions = []
         for i in range(logits.shape[0]):
             predictions.append(logits[i].argmax(dim=-1).detach().cpu().numpy())
 
-        if len(predictions) == 1:
+        if not isinstance(sequence_or_inputs, list):
             outputs = {
                 "predictions": predictions[0],
                 "logits": logits[0],
                 "last_hidden_state": last_hidden_state[0],
             }
         else:
             outputs = {
@@ -75,27 +73,35 @@
 
         return outputs
 
     def inference(self, sequence_or_inputs, **kwargs):
         inputs = self.tokenizer(sequence_or_inputs, return_tensors="pt", **kwargs)
         inputs = inputs.to(self.model.device)
 
-        outputs = self(inputs)
-        logits = outputs["logits"]
-        last_hidden_state = outputs["last_hidden_state"]
+        with torch.no_grad():
+            outputs = self(inputs)
+        logits = outputs["logits"][:, 1:-1:, :]
+        last_hidden_state = outputs["last_hidden_state"][:, 1:-1:, :]
 
         predictions = []
         for i in range(logits.shape[0]):
             i_logits = logits[i][
                 : inputs["input_ids"][i].ne(self.tokenizer.pad_token_id).sum().item()
-            ][1:-1]
-            prediction = self.tokenizer.decode(i_logits.argmax(dim=-1))
-            predictions.append(prediction)
+            ]
+            prediction = self.tokenizer.decode(i_logits.argmax(dim=-1)).replace(" ", "")
+            if (
+                torch.sum(
+                    inputs["input_ids"][i] == self.tokenizer.convert_tokens_to_ids("U")
+                )
+                > 0
+            ):
+                prediction = prediction.replace("U", "T")
+            predictions.append(list(prediction))
 
-        if len(predictions) == 1:
+        if not isinstance(sequence_or_inputs, list):
             outputs = {
                 "predictions": predictions[0],
                 "logits": logits[0],
                 "last_hidden_state": last_hidden_state[0],
             }
         else:
             outputs = {
```

## omnigenome/src/model/regression/model.py

```diff
@@ -4,14 +4,15 @@
 # author: YANG, HENG <hy345@exeter.ac.uk> (杨恒)
 # github: https://github.com/yangheng95
 # huggingface: https://huggingface.co/yangheng
 # google scholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
 # Copyright (C) 2019-2024. All Rights Reserved.
 
 import torch
+from transformers import BatchEncoding
 from transformers.models.bert.modeling_bert import BertPooler
 
 from ...abc.abstract_model import OmniGenomeModel
 from ...abc.abstract_model import extract_last_hidden_state
 
 
 class OmniGenomeEncoderModelForTokenRegression(OmniGenomeModel):
@@ -26,65 +27,56 @@
         last_hidden_state = self.dropout(last_hidden_state)
         last_hidden_state = self.activation(last_hidden_state)
         logits = self.classifier(last_hidden_state)
         outputs = {"logits": logits, "last_hidden_state": last_hidden_state}
         return outputs
 
     def predict(self, sequence_or_inputs, **kwargs):
-        if isinstance(sequence_or_inputs, str):
+        if not isinstance(sequence_or_inputs, BatchEncoding) and not isinstance(
+            sequence_or_inputs, dict
+        ):
             inputs = self.tokenizer(sequence_or_inputs, return_tensors="pt", **kwargs)
         else:
             inputs = sequence_or_inputs
+        inputs = inputs.to(self.model.device)
 
-        if len(inputs["input_ids"].shape) == 1:
-            for col in inputs:
-                inputs[col] = inputs[col].to(self.model.device).unsqueeze(0)
-        else:
-            for col in inputs:
-                inputs[col] = inputs[col].to(self.model.device)
-
-        outputs = self(inputs)
+        with torch.no_grad():
+            outputs = self(inputs)
         logits = outputs["logits"]
         last_hidden_state = outputs["last_hidden_state"]
 
         predictions = []
         for i in range(logits.shape[0]):
             predictions.append(logits[i].detach().cpu().numpy())
 
-        if len(predictions) == 1:
-            outputs = {
-                "predictions": predictions[0],
-                "logits": logits[0],
-                "last_hidden_state": last_hidden_state[0],
-            }
-        else:
-            outputs = {
-                "predictions": predictions,
-                "logits": logits,
-                "last_hidden_state": last_hidden_state,
-            }
+        outputs = {
+            "predictions": predictions,
+            "logits": logits,
+            "last_hidden_state": last_hidden_state,
+        }
 
         return outputs
 
     def inference(self, sequence_or_inputs, **kwargs):
         inputs = self.tokenizer(sequence_or_inputs, return_tensors="pt", **kwargs)
         inputs = inputs.to(self.model.device)
 
-        outputs = self(inputs)
-        logits = outputs["logits"]
-        last_hidden_state = outputs["last_hidden_state"]
+        with torch.no_grad():
+            outputs = self(inputs)
+        logits = outputs["logits"][:, 1:-1:, :]
+        last_hidden_state = outputs["last_hidden_state"][:, 1:-1:, :]
 
         predictions = []
         for i in range(logits.shape[0]):
             i_logits = logits[i][
                 : inputs["input_ids"][i].ne(self.config.pad_token_id).sum().item()
-            ][1:-1]
+            ]
             predictions.append(i_logits.detach().cpu().numpy())
 
-        if len(predictions) == 1:
+        if not isinstance(sequence_or_inputs, list):
             outputs = {
                 "predictions": predictions[0],
                 "logits": logits[0],
                 "last_hidden_state": last_hidden_state[0],
             }
         else:
             outputs = {
@@ -125,62 +117,53 @@
         last_hidden_state = self.activation(last_hidden_state)
         pooled_output = self.pooler(last_hidden_state)
         logits = self.classifier(pooled_output)
         outputs = {"logits": logits, "last_hidden_state": last_hidden_state}
         return outputs
 
     def predict(self, sequence_or_inputs, **kwargs):
-        if isinstance(sequence_or_inputs, str):
+        if not isinstance(sequence_or_inputs, BatchEncoding) and not isinstance(
+            sequence_or_inputs, dict
+        ):
             inputs = self.tokenizer(sequence_or_inputs, return_tensors="pt", **kwargs)
         else:
             inputs = sequence_or_inputs
+        inputs = inputs.to(self.model.device)
 
-        if len(inputs["input_ids"].shape) == 1:
-            for col in inputs:
-                inputs[col] = inputs[col].to(self.model.device).unsqueeze(0)
-        else:
-            for col in inputs:
-                inputs[col] = inputs[col].to(self.model.device)
-
-        outputs = self(inputs)
+        with torch.no_grad():
+            outputs = self(inputs)
         logits = outputs["logits"]
         last_hidden_state = outputs["last_hidden_state"]
 
         predictions = []
         for i in range(logits.shape[0]):
             predictions.append(logits[i][0].item())
 
-        if len(predictions) == 1:
-            outputs = {
-                "predictions": predictions[0],
-                "logits": logits[0],
-                "last_hidden_state": last_hidden_state[0],
-            }
-        else:
-            outputs = {
-                "predictions": predictions,
-                "logits": logits,
-                "last_hidden_state": last_hidden_state,
-            }
+        outputs = {
+            "predictions": predictions,
+            "logits": logits,
+            "last_hidden_state": last_hidden_state,
+        }
 
         return outputs
 
     def inference(self, sequence_or_inputs, **kwargs):
         inputs = self.tokenizer(sequence_or_inputs, return_tensors="pt", **kwargs)
         inputs = inputs.to(self.model.device)
 
-        outputs = self(inputs)
+        with torch.no_grad():
+            outputs = self(inputs)
         logits = outputs["logits"]
         last_hidden_state = outputs["last_hidden_state"]
 
         predictions = []
         for i in range(logits.shape[0]):
             predictions.append(logits[i][0].item())
 
-        if len(predictions) == 1:
+        if not isinstance(sequence_or_inputs, list):
             outputs = {
                 "predictions": predictions[0],
                 "logits": logits[0],
                 "last_hidden_state": last_hidden_state[0],
             }
         else:
             outputs = {
```

## omnigenome/src/tokenizer/bpe_tokenizer.py

```diff
@@ -4,14 +4,15 @@
 # author: YANG, HENG <hy345@exeter.ac.uk> (杨恒)
 # github: https://github.com/yangheng95
 # huggingface: https://huggingface.co/yangheng
 # google scholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
 # Copyright (C) 2019-2024. All Rights Reserved.
 
 from ..abc.abstract_tokenizer import OmniGenomeTokenizer
+from transformers import AutoTokenizer
 
 
 def is_bpe_tokenization(tokens, threshold=0.1):
     if not tokens:
         return False
 
     bpe_endings_count = sum(
@@ -24,15 +25,15 @@
 
     return bpe_ratio >= threshold
 
 
 class OmniBPETokenizer(OmniGenomeTokenizer):
     def __init__(self, base_tokenizer=None, **kwargs):
         super(OmniBPETokenizer, self).__init__(base_tokenizer, **kwargs)
-        self.metadata["tokenizer_name"] = "BPETokenizer"
+        self.metadata["tokenizer_name"] = self.__class__.__name__
 
     def __call__(self, sequence, **kwargs):
         sequences = self.tokenize(sequence)
 
         if not is_bpe_tokenization(sequences):
             raise ValueError("The tokenizer seems not to be a BPE tokenizer.")
 
@@ -43,14 +44,21 @@
             max_length=self.max_length,
             return_tensors="pt",
             **kwargs
         )
 
         return tokenized_inputs
 
+    @staticmethod
+    def from_pretrained(model_name_or_path, **kwargs):
+        self = OmniBPETokenizer(
+            AutoTokenizer.from_pretrained(model_name_or_path, **kwargs)
+        )
+        return self
+
     def tokenize(self, sequence, **kwargs):
         return self.base_tokenizer.tokenize(sequence)
 
     def encode(self, sequence, **kwargs):
         assert hasattr(
             self.base_tokenizer, "bpe"
         ), "The base tokenizer must be a BPE tokenizer."
```

## omnigenome/src/tokenizer/kmers_tokenizer.py

```diff
@@ -5,37 +5,42 @@
 # github: https://github.com/yangheng95
 # huggingface: https://huggingface.co/yangheng
 # google scholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
 # Copyright (C) 2019-2024. All Rights Reserved.
 import warnings
 
 from ..abc.abstract_tokenizer import OmniGenomeTokenizer
+from transformers import AutoTokenizer
 
 
 class OmniKmersTokenizer(OmniGenomeTokenizer):
     def __init__(self, base_tokenizer=None, k=3, overlap=0, max_length=512, **kwargs):
         super(OmniKmersTokenizer, self).__init__(base_tokenizer, **kwargs)
         self.k = k
         self.overlap = overlap
         self.max_length = max_length
-        self.metadata["tokenizer_name"] = "KmersTokenizer"
+        self.metadata["tokenizer_name"] = self.__class__.__name__
 
     def __call__(self, sequence, **kwargs):
+        if self.u2t:
+            sequence = sequence.replace("U", "T")
+        if self.add_whitespace:
+            sequence = " ".join(list(sequence))
         sequence_tokens = self.tokenize(sequence)
         tokenized_inputs = {
             "input_ids": [],
             "attention_mask": [],
         }
         bos_id, eos_id = self.base_tokenizer("")["input_ids"]
 
         for tokens in sequence_tokens:
             tokenized_inputs["input_ids"].append(
                 [bos_id]
                 + self.base_tokenizer.convert_tokens_to_ids(
-                    tokens[: self.max_length - 2]
+                    tokens[: kwargs.get("max_length", self.max_length) - 2]
                 )
                 + [eos_id]
             )
             tokenized_inputs["attention_mask"].append(
                 [1] * len(tokenized_inputs["input_ids"][-1])
             )
 
@@ -50,24 +55,31 @@
             max_length=self.max_length,
             pad_to_multiple_of=self.max_length,
             return_attention_mask=True,
             return_tensors="pt",
         )
         return tokenized_inputs
 
+    @staticmethod
+    def from_pretrained(model_name_or_path, **kwargs):
+        self = OmniKmersTokenizer(
+            AutoTokenizer.from_pretrained(model_name_or_path, **kwargs)
+        )
+        return self
+
     def tokenize(self, sequence, **kwargs):
         if isinstance(sequence, str):
             sequences = [sequence]
         else:
             sequences = sequence
 
         sequence_tokens = []
         for i in range(len(sequences)):
             tokens = []
-            for j in range(0, len(sequences[i]), self.overlap):
+            for j in range(0, len(sequences[i]), self.k - self.overlap):
                 tokens.append(sequences[i][j : j + self.k])
 
             sequence_tokens.append(tokens)
 
         return sequence_tokens
 
     def encode(self, input_ids, **kwargs):
```

## omnigenome/src/tokenizer/single_nucleotide_tokenizer.py

```diff
@@ -2,61 +2,68 @@
 # file: single_nucleotide_tokenizer.py
 # time: 18:05 08/04/2024
 # author: YANG, HENG <hy345@exeter.ac.uk> (杨恒)
 # github: https://github.com/yangheng95
 # huggingface: https://huggingface.co/yangheng
 # google scholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
 # Copyright (C) 2019-2024. All Rights Reserved.
-import pickle
+
 import warnings
 
 from ..abc.abstract_tokenizer import OmniGenomeTokenizer
+from transformers import AutoTokenizer
 
 
 class OmniSingleNucleotideTokenizer(OmniGenomeTokenizer):
     def __init__(self, base_tokenizer=None, **kwargs):
         super(OmniSingleNucleotideTokenizer, self).__init__(base_tokenizer, **kwargs)
-        self.metadata["tokenizer_name"] = "SingleNucleotideTokenizer"
+        self.metadata["tokenizer_name"] = self.__class__.__name__
 
     def __call__(self, sequence, **kwargs):
         sequence_tokens = self.tokenize(sequence)
         tokenized_inputs = {
             "input_ids": [],
             "attention_mask": [],
         }
         bos_id, eos_id = self.base_tokenizer("")["input_ids"]
 
         for tokens in sequence_tokens:
             tokenized_inputs["input_ids"].append(
                 [bos_id]
                 + self.base_tokenizer.convert_tokens_to_ids(
-                    tokens[: self.max_length - 2]
+                    tokens[: kwargs.get("max_length", self.max_length) - 2]
                 )
                 + [eos_id]
             )
             tokenized_inputs["attention_mask"].append(
                 [1] * len(tokenized_inputs["input_ids"][-1])
             )
 
         for i, ids in enumerate(tokenized_inputs["input_ids"]):
             if ids.count(self.base_tokenizer.unk_token_id) / len(ids) > 0.1:
                 warnings.warn(
                     f"Unknown tokens are more than 10% in the {i}th sequence, please check the tokenization process."
                 )
-
+        max_length = max(len(ids) for ids in tokenized_inputs["input_ids"])
         tokenized_inputs = self.base_tokenizer.pad(
             tokenized_inputs,
-            padding="max_length",
-            max_length=self.max_length,
-            pad_to_multiple_of=self.max_length,
-            return_attention_mask=True,
+            padding=kwargs.get("padding", "max_length"),
+            max_length=min(max_length, kwargs.get("max_length", 512)),
+            return_attention_mask=kwargs.get("return_attention_mask", True),
             return_tensors="pt",
         )
         return tokenized_inputs
 
+    @staticmethod
+    def from_pretrained(model_name_or_path, **kwargs):
+        self = OmniSingleNucleotideTokenizer(
+            AutoTokenizer.from_pretrained(model_name_or_path, **kwargs)
+        )
+        return self
+
     def tokenize(self, sequence, **kwargs):
         if isinstance(sequence, str):
             sequences = [sequence]
         else:
             sequences = sequence
 
         sequence_tokens = []
@@ -69,34 +76,7 @@
         return self.base_tokenizer.encode(sequence, **kwargs)
 
     def decode(self, sequence, **kwargs):
         return self.base_tokenizer.decode(sequence, **kwargs)
 
     def encode_plus(self, sequence, **kwargs):
         return self.base_tokenizer.encode_plus(sequence, **kwargs)
-
-
-if __name__ == "__main__":
-    from transformers import AutoTokenizer
-
-    # RNA = "ACGUAGGUAUCGUAGA"
-    # # base_tokenizer_name = 'bert-base-cased'
-    # base_tokenizer_name = "facebook/esm2_t12_35M_UR50D"
-    # base_tokenizer = AutoTokenizer.from_pretrained(base_tokenizer_name)
-    # tokenizer = KmersTokenizer(base_tokenizer)
-    # tokens = tokenizer.tokenize(RNA)
-    # print(tokens)
-    # tokenized_inputs = tokenizer(RNA)
-    # print(tokenized_inputs)
-
-    RNA = "ACGUAGGUAUCGUAGA"
-    base_tokenizer_name = "bert-base-cased"
-    # base_tokenizer_name = "facebook/esm2_t12_35M_UR50D"
-    base_tokenizer = AutoTokenizer.from_pretrained(base_tokenizer_name)
-    tokenizer = OmniSingleNucleotideTokenizer(base_tokenizer, max_length=512)
-    tokens = tokenizer.tokenize(RNA)
-    print(tokens)
-    tokenized_inputs = tokenizer(RNA)
-    print(tokenized_inputs)
-    pickle.dump(tokenizer, open("tokenizer.og.pkl", "wb"))
-    tokenizer = pickle.load(open("tokenizer.og.pkl", "rb"))
-    tokenized_inputs = tokenizer(RNA)
```

## omnigenome/src/trainer/trainer.py

```diff
@@ -101,15 +101,16 @@
 
     def evaluate(self):
         valid_metrics = {}
         with torch.no_grad():
             self.model.eval()
             val_truth = []
             val_preds = []
-            for batch in self.eval_loader:
+            it = tqdm(self.eval_loader, desc="Evaluating")
+            for batch in it:
                 batch.to(self.device)
                 predictions = self.model.predict(batch)["predictions"]
                 val_truth.append(batch["labels"].detach().cpu().numpy())
                 val_preds.append(np.array(predictions))
 
             val_truth = np.concatenate(val_truth)
             val_preds = np.concatenate(val_preds)
@@ -145,7 +146,18 @@
         raise NotImplementedError(
             "The compute_metrics() function should be implemented for your model."
             " It should return a dictionary of metrics."
         )
 
     def save_model(self, path, overwrite=False, **kwargs):
         self.model.save(path, overwrite, **kwargs)
+
+    def _reload_state_dict(self, path):
+        self.optimizer.load_state_dict(torch.load(path))
+        self.model.load_state_dict(torch.load(path), map_location=self.device)
+
+    def _save_state_dict(self, path=None):
+        if path is None:
+            path = "init_state_dict.pt"
+        self.model.to("cpu")
+        torch.save(self.model.state_dict(), path)
+        self.model.to(self.device)
```

## omnigenome/utility/model_hub/model_hub.py

```diff
@@ -51,20 +51,18 @@
         model_lib = importlib.import_module(metadata["library_name"].lower()).model
         model_cls = getattr(model_lib, metadata["model_cls"])
 
         with open(f"{path}/tokenizer.pkl", "rb") as f:
             tokenizer = dill.load(f)
 
         model = model_cls(config, base_model, tokenizer, **kwargs)
-
         with open(f"{path}/pytorch_model.bin", "rb") as f:
             model.load_state_dict(
-                torch.load(f, map_location=kwargs.get("device", "cpu")), strict=False
+                torch.load(f, map_location=kwargs.get("device", "cpu")), strict=True
             )
-            model.metadata.update(metadata)
         return model
 
     @staticmethod
     def save(self, path, overwrite=False, **kwargs):
         import dill
 
         if os.path.exists(path) and not overwrite:
```

## omnigenome/utility/pipeline_hub/pipeline.py

```diff
@@ -66,32 +66,20 @@
     def to(self, device):
         self.model.to(device)
         self.device = device
 
     def init_pipeline(self, *, model_name_or_path, tokenizer=None, **kwargs):
         trust_remote_code = kwargs.get("trust_remote_code", True)
         try:  # for the models saved by OmniGenome and served by the model hub
-            self.model, self.tokenizer = ModelHub.load(model_name_or_path, **kwargs)
-            return self
+            self.model = ModelHub.load(model_name_or_path, **kwargs)
+            self.tokenizer = self.model.tokenizer
+            self.metadata.update(self.model.metadata)
         except Exception as e:
             print(f"Fail to load the model from the model hub, the error is: {e}")
 
-        try:  # for the models saved by the OmniGenome and located in the local file system
-            self.model, self.tokenizer = ModelHub.load_model_and_tokenizer(
-                model_name_or_path, **kwargs
-            )
-            return self
-        except Exception as e:
-            print(
-                f"Fail to load the model from the local file system, the error is: {e}"
-            )
-
-        if isinstance(
-            model_name_or_path, str
-        ):  # for the models from the Hugging Face model hub
             config = AutoConfig.from_pretrained(
                 model_name_or_path, trust_remote_code=trust_remote_code
             )
             if tokenizer is None:
                 tokenizer = AutoTokenizer.from_pretrained(
                     model_name_or_path, trust_remote_code=trust_remote_code
                 )
@@ -99,16 +87,15 @@
                 model_name_or_path,
                 config=config,
                 tokenizer=tokenizer,
                 trust_remote_code=trust_remote_code,
                 **kwargs,
             )
             self.tokenizer = self.model.tokenizer
-
-        self.metadata = self.model.metadata
+            self.metadata.update(self.model.metadata)
         fprint(f"The pipeline has been initialized from {model_name_or_path}.")
         return self
 
     def train(self, datasets: dict = None, trainer=None, **kwargs):
         if trainer is not None:
             assert isinstance(trainer, Trainer)
             self.trainer = trainer
@@ -139,17 +126,19 @@
         with open(f"{path}/tokenizer.pkl", "rb") as f:
             tokenizer = dill.load(f)
         with open(f"{path}/trainer.pkl", "rb") as f:
             trainer = dill.load(f)
         model = ModelHub.load(path, local_only=local_only, **kwargs)
         model.metadata.update(metadata)
         pipeline = Pipeline(
-            name=pipeline_name_or_path
-            if kwargs.get("name") is None
-            else kwargs.get("name"),
+            name=(
+                pipeline_name_or_path
+                if kwargs.get("name") is None
+                else kwargs.get("name")
+            ),
             model_name_or_path=model,
             tokenizer=tokenizer,
             datasets=datasets,
             trainer=trainer,
             **kwargs,
         )
         return pipeline
```

## Comparing `OmniGenome-0.0.1a0.dist-info/METADATA` & `OmniGenome-0.0.1b1.dist-info/METADATA`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: OmniGenome
-Version: 0.0.1a0
+Version: 0.0.1b1
 Summary: OmniGenome: A comprehensive toolkit for genome analysis.
 Home-page: https://github.com/yangheng95/OmniGenome
 Author: Yang, Heng
 Author-email: hy345@exeter.ac.uk
 License: MIT
 Requires-Python: >=3.9
 Description-Content-Type: text/markdown
@@ -15,12 +15,13 @@
 Requires-Dist: termcolor
 Requires-Dist: gitpython
 Requires-Dist: transformers (>=4.37.0)
 Requires-Dist: torch (>=1.0.0)
 Requires-Dist: sentencepiece
 Requires-Dist: protobuf (<4.0.0)
 Requires-Dist: pandas
+Requires-Dist: viennarna
 Provides-Extra: dev
 Requires-Dist: dill ; extra == 'dev'
 Requires-Dist: pytest ; extra == 'dev'
 
 # OmniGenome: A Comprehensive Toolkit of Genomic Modeling and Benchmarking
```

## Comparing `OmniGenome-0.0.1a0.dist-info/RECORD` & `OmniGenome-0.0.1b1.dist-info/RECORD`

 * *Files 27% similar despite different names*

```diff
@@ -1,97 +1,53 @@
-omnigenome/__init__.py,sha256=I2KI1FWk8THl84bvxgS1GscK8zDgezET-Gfz5L1Zt9g,4823
-omnigenome/_src/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-omnigenome/_src/abc/__init__.py,sha256=ir0dYJyibYfFlZaRG4PzPgqNmhHReqrvFRg8KzmFc4w,342
-omnigenome/_src/abc/abstract_dataset.py,sha256=jx55P_eIfaguQ8gvJgOFrXdptJXXyRjVs6lvIWnRgvw,10028
-omnigenome/_src/abc/abstract_metric.py,sha256=xGHNFEcnSwZcnanEAUs9__cNLPzgtN5r26xc6x2ehWU,1773
-omnigenome/_src/abc/abstract_model.py,sha256=r4KsHstd2Bw7dOuxwgm28OPxoBjzZL8t0kQwgJ3Zqt0,10266
-omnigenome/_src/abc/abstract_tokenizer.py,sha256=pym6Hawcw7wLkfsEOl3fswuEQka-INqb-zs3luIYSk8,1884
-omnigenome/_src/config/__init__.py,sha256=Tg4SJYoUQWO4OQz3nTaU47Ms_mgPC5ArZ8vLRF2Ziyg,342
-omnigenome/_src/config/config.py,sha256=lHZKuwZpm7RdN2Stcqgk6Qc3vRiX0CrQxhQbTnRTQvI,7654
-omnigenome/_src/config/config_check.py,sha256=PWusimXDSLJyOivrVRMnMFolMI16ttAmdDSzb5_VrTE,971
-omnigenome/_src/dataset/__init__.py,sha256=91o171BfDCddZaYa0QNQ3YYbXycNPTZ6ByyCw2PeK-Y,689
-omnigenome/_src/dataset/omnigenome_dataset.py,sha256=zFBwxok6IOuoj0PrCKExkVtEpLnmB5rE2C9hqwjWXFo,8801
-omnigenome/_src/metric/__init__.py,sha256=3dR3fGcyYGCqFAY7PLt2eewWuQmCsDfwecNeOoWC9M4,493
-omnigenome/_src/metric/classification_metric.py,sha256=xIsHC-WuqtlvDjqAz0vXPoL4E6k5NduwvvVyrXfR7Zg,2658
-omnigenome/_src/metric/ranking_metric.py,sha256=1Bg5AxtHsNLJfAEL_xjHC5xCXxEnRF7uvKHkgUa8XQ0,2248
-omnigenome/_src/metric/regression_metric.py,sha256=NtELLQcn0EUAwjgfhBzUgJ8vHEfeHjJeKjaWQe0jIKU,2626
-omnigenome/_src/misc/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-omnigenome/_src/misc/utils.py,sha256=1zsimLH9FjOREHhCw2eYrOG1_VPE17NjF1J290ukRQM,5497
-omnigenome/_src/model/__init__.py,sha256=y6dJ-8OUz3i9b9jg_ydsljeLfRyDwU6zzithhslFtBY,444
-omnigenome/_src/model/classiifcation/__init__.py,sha256=dPugrswvrM58nVxS5FlCBnx2meuTPY9SzOA4YLrP-l4,342
-omnigenome/_src/model/classiifcation/model.py,sha256=P2mTFmwVx4j2s34OZiGSderyYtglbKAzoxhC3qKEOMc,18734
-omnigenome/_src/model/mlm/__init__.py,sha256=0-S-GmzGIx7qB7Aixh_KovHxAcvKHgCZJcxYHifjZ-A,342
-omnigenome/_src/model/mlm/model.py,sha256=CDZoWzwl4VjnYKR38eGZHC9pPRLT2ObCYyolZHZ3CWw,4136
-omnigenome/_src/model/regression/__init__.py,sha256=P_xAnv8ydAA49O2NeRBf7TEU78OZwGE5xEAT8V2FdoQ,342
-omnigenome/_src/model/regression/model.py,sha256=9GSRx3k34Xg28JRCKqRnyuahVVQcvtwrUGRPUhGTpGw,18876
-omnigenome/_src/model/seq2seq/__init__.py,sha256=sP2vP_2XzMI80eWAbTT5cboKY9lxjPxe-Mtfiu_vK-A,342
-omnigenome/_src/model/seq2seq/model.py,sha256=IMAIctPS7MovvKPcb5bp5RdaBrWFBG3e_mWs8Z6bwjQ,339
-omnigenome/_src/tokenizer/__init__.py,sha256=v6zrDn76MkBHDOsX8VNftiP_NntlmKggl5TKIvNVpFU,512
-omnigenome/_src/tokenizer/bpe_tokenizer.py,sha256=tfGJHdeeKqJg0QUpsCvcjo98KYol5QGyCSdt7MNe8y8,2534
-omnigenome/_src/tokenizer/kmers_tokenizer.py,sha256=DFNEtyZDCX7eRYQvv2Q4S7DdkX8fdhyh04cgm2sTNXI,3857
-omnigenome/_src/tokenizer/single_nucleotide_tokenizer.py,sha256=u1KjxqudwJikwPnzlLcFBBmbewIRib9Rlpb_w0inikY,3786
-omnigenome/_src/trainer/__init__.py,sha256=oM-Jh4_-RDi0j2omG-7Vxwx3LAR_LAgc2gAERCWMi4Q,409
-omnigenome/_src/trainer/hf_trainer.py,sha256=jPHmXEDgA-OO8wgA_aKLChQcfEw4Dtm1AOFhzyvBkBk,1092
-omnigenome/_src/trainer/trainer.py,sha256=5Euyqcl8b3xyKqR6ZpsExeLFOMwI6EkW2XyrqRCC_Bc,5958
+omnigenome/__init__.py,sha256=JSEfEOgLncBQae1-yV-LNAw6gAyvGqfmMRAJcpw4e_Y,4822
 omnigenome/bench/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 omnigenome/bench/auto_bench/__init__.py,sha256=JEdZznP93R9CQ2zZCXPOIxN0OizNa2RNa30EDkRCz3o,342
 omnigenome/bench/auto_bench/auto_bench.py,sha256=m-CnGrbGJJzAOkr4uAKqVm3ssnXEuN3IdEiY7V_wokU,472
 omnigenome/bench/bench_hub/__init__.py,sha256=JEdZznP93R9CQ2zZCXPOIxN0OizNa2RNa30EDkRCz3o,342
 omnigenome/bench/bench_hub/bench_hub.py,sha256=Up4JjqGgKH1jtYPRwvb4w1UpxE6W3FxlUEuUcEelyKI,439
 omnigenome/src/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 omnigenome/src/abc/__init__.py,sha256=ir0dYJyibYfFlZaRG4PzPgqNmhHReqrvFRg8KzmFc4w,342
-omnigenome/src/abc/abstract_dataset.py,sha256=jx55P_eIfaguQ8gvJgOFrXdptJXXyRjVs6lvIWnRgvw,10028
+omnigenome/src/abc/abstract_dataset.py,sha256=3HkyJB3Mxxz5gERskMddOdyAMPS73GbbSL-9Hjc-WUc,10223
 omnigenome/src/abc/abstract_metric.py,sha256=xGHNFEcnSwZcnanEAUs9__cNLPzgtN5r26xc6x2ehWU,1773
-omnigenome/src/abc/abstract_model.py,sha256=JS4ytYuTnFh8CqZkzvbuZDS-IPp8LEwOJINLBpYIFvs,10238
-omnigenome/src/abc/abstract_tokenizer.py,sha256=pym6Hawcw7wLkfsEOl3fswuEQka-INqb-zs3luIYSk8,1884
+omnigenome/src/abc/abstract_model.py,sha256=_5vf4MI1WYlNTfmwmrhaYu5XLOAT2NCNwqKQsGcQxuc,10338
+omnigenome/src/abc/abstract_tokenizer.py,sha256=S-kaoJoe1ozVe6xDvGTbxghb_ov1DiTV-pMPijbI_XA,2501
 omnigenome/src/config/__init__.py,sha256=Tg4SJYoUQWO4OQz3nTaU47Ms_mgPC5ArZ8vLRF2Ziyg,342
 omnigenome/src/config/config.py,sha256=lHZKuwZpm7RdN2Stcqgk6Qc3vRiX0CrQxhQbTnRTQvI,7654
 omnigenome/src/config/config_check.py,sha256=PWusimXDSLJyOivrVRMnMFolMI16ttAmdDSzb5_VrTE,971
 omnigenome/src/dataset/__init__.py,sha256=91o171BfDCddZaYa0QNQ3YYbXycNPTZ6ByyCw2PeK-Y,689
-omnigenome/src/dataset/omnigenome_dataset.py,sha256=zFBwxok6IOuoj0PrCKExkVtEpLnmB5rE2C9hqwjWXFo,8801
+omnigenome/src/dataset/omnigenome_dataset.py,sha256=b7L3067as6k_B_0GAjdl6GTp_iXoWu4_pjujP27Cs5Y,8737
 omnigenome/src/metric/__init__.py,sha256=3dR3fGcyYGCqFAY7PLt2eewWuQmCsDfwecNeOoWC9M4,493
 omnigenome/src/metric/classification_metric.py,sha256=xIsHC-WuqtlvDjqAz0vXPoL4E6k5NduwvvVyrXfR7Zg,2658
 omnigenome/src/metric/ranking_metric.py,sha256=1Bg5AxtHsNLJfAEL_xjHC5xCXxEnRF7uvKHkgUa8XQ0,2248
 omnigenome/src/metric/regression_metric.py,sha256=NtELLQcn0EUAwjgfhBzUgJ8vHEfeHjJeKjaWQe0jIKU,2626
 omnigenome/src/misc/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-omnigenome/src/misc/utils.py,sha256=1zsimLH9FjOREHhCw2eYrOG1_VPE17NjF1J290ukRQM,5497
+omnigenome/src/misc/utils.py,sha256=UtFQrJiyE040nTtbMxB753osbCD7SWhTnRbnMtjqKlg,5586
 omnigenome/src/model/__init__.py,sha256=UhfIzs2IQRWJUPNN9YzwVfSEbTIDnTRTYBuyUO_GKCo,470
 omnigenome/src/model/classiifcation/__init__.py,sha256=dPugrswvrM58nVxS5FlCBnx2meuTPY9SzOA4YLrP-l4,342
-omnigenome/src/model/classiifcation/model.py,sha256=P2mTFmwVx4j2s34OZiGSderyYtglbKAzoxhC3qKEOMc,18734
+omnigenome/src/model/classiifcation/model.py,sha256=BQwi34SR00u0Qz41z43rqGhtRLpbAf6v9xrtN9J1G28,18174
 omnigenome/src/model/mlm/__init__.py,sha256=0-S-GmzGIx7qB7Aixh_KovHxAcvKHgCZJcxYHifjZ-A,342
-omnigenome/src/model/mlm/model.py,sha256=CDZoWzwl4VjnYKR38eGZHC9pPRLT2ObCYyolZHZ3CWw,4136
+omnigenome/src/model/mlm/model.py,sha256=huAd2ZnvEPQU95OHaIXCZx-8HIkocc5wf4fqQe9HZfo,4434
 omnigenome/src/model/regression/__init__.py,sha256=P_xAnv8ydAA49O2NeRBf7TEU78OZwGE5xEAT8V2FdoQ,342
-omnigenome/src/model/regression/model.py,sha256=9GSRx3k34Xg28JRCKqRnyuahVVQcvtwrUGRPUhGTpGw,18876
+omnigenome/src/model/regression/model.py,sha256=9OjGE-_2hO8eHtec2yjHcunhXGDTv-0jtcPE6Xi2K9E,18316
 omnigenome/src/model/seq2seq/__init__.py,sha256=sP2vP_2XzMI80eWAbTT5cboKY9lxjPxe-Mtfiu_vK-A,342
 omnigenome/src/model/seq2seq/model.py,sha256=CQq2dzkqLTxKwsMyNKXMkI0tXEV9askWPG3QeNcPypM,601
 omnigenome/src/tokenizer/__init__.py,sha256=v6zrDn76MkBHDOsX8VNftiP_NntlmKggl5TKIvNVpFU,512
-omnigenome/src/tokenizer/bpe_tokenizer.py,sha256=tfGJHdeeKqJg0QUpsCvcjo98KYol5QGyCSdt7MNe8y8,2534
-omnigenome/src/tokenizer/kmers_tokenizer.py,sha256=DFNEtyZDCX7eRYQvv2Q4S7DdkX8fdhyh04cgm2sTNXI,3857
-omnigenome/src/tokenizer/single_nucleotide_tokenizer.py,sha256=u1KjxqudwJikwPnzlLcFBBmbewIRib9Rlpb_w0inikY,3786
+omnigenome/src/tokenizer/bpe_tokenizer.py,sha256=9VS3k3TEBdxArRlk6q_t4BlLOrZcdG_iQjzez8VbaMc,2799
+omnigenome/src/tokenizer/kmers_tokenizer.py,sha256=GhOSCfF5oNg18FqAj-exIZKBLYAM2p8gJ760O6Y-MII,4312
+omnigenome/src/tokenizer/single_nucleotide_tokenizer.py,sha256=7Fvy5QxhjqkBVDpSaXE7vIIkjhVSwvK4Pfo1FF-7_iI,3132
 omnigenome/src/trainer/__init__.py,sha256=oM-Jh4_-RDi0j2omG-7Vxwx3LAR_LAgc2gAERCWMi4Q,409
 omnigenome/src/trainer/hf_trainer.py,sha256=jPHmXEDgA-OO8wgA_aKLChQcfEw4Dtm1AOFhzyvBkBk,1092
-omnigenome/src/trainer/trainer.py,sha256=0oNzCZQPKvoUEum71gX5JThw_3l9d5EcjX1XIzYuxFA,5400
+omnigenome/src/trainer/trainer.py,sha256=bAOH_OYrBNaxkzQOBxLsdN0OurRvQrbCWFYp0x0Hl8o,5857
 omnigenome/utility/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 omnigenome/utility/hub_utils.py,sha256=2OvRHsj_q99185_-lY8x2-vISStVMfwzbcHieKTDwVg,10682
 omnigenome/utility/model_hub/__init__.py,sha256=p_rxaidCUZTW_Ws4q8B36RMiqK_wnRRsjtxA4CFp6lc,342
-omnigenome/utility/model_hub/model_hub.py,sha256=bzwmU07VzLr85C0uWnlsEtyy2f-u_PFMh0f0hk9H66I,3689
+omnigenome/utility/model_hub/model_hub.py,sha256=LRtZS6yd0ZPgGB6Ec_mvcaBixK5LSHXJauldShuKjsQ,3641
 omnigenome/utility/pipeline_hub/__init__.py,sha256=5EQKxssQeyl195CeJG31QuwjTt8vrUWbBjim0vzzF1I,342
-omnigenome/utility/pipeline_hub/pipeline.py,sha256=pAnXkHZd7_uGdDaGnfya0m8Q2H3QxF8dafihaMXNvtA,6155
+omnigenome/utility/pipeline_hub/pipeline.py,sha256=lU6nS0C7cSHdzbRvSw5iLYtMCCPqcBQzMOhiHywA5ZI,5744
 omnigenome/utility/pipeline_hub/pipeline_hub.py,sha256=XJOHduSM-T7olGiqdTp-fg_QKQ9-y40IGg-IcoX24-0,883
 tutorials/__init__.py,sha256=AcY3fJ8Q6uJsjVR_-4p4SKe5QMHopNAOIC-OixAbSxE,342
 tutorials/codonbert_adapter.py,sha256=AQ0vMk_xKeYNJ66z4pn-RD14Lrkxi-kupEJhBcLF0Qk,1660
-tutorials/mRNA_test.py,sha256=pQGCAkRv_5EOa8G828S4Y7YNVgeHUKWr-1lPGuqN4SQ,4710
-tutorials/mlm_service_test.py,sha256=UzWZWASrLrKVsEQuD4Q1CYPUM9FST5heMLiDx4yPKYU,3463
-tutorials/pc_test.py,sha256=byQAxA6mHm2JnYPx7RRRidpmFRNQn3JQdcS6b6vtY78,3496
-tutorials/psp_test.py,sha256=AXyODtN6Oe3qTCGuOfEJaygvG2AI_aFKObae0WXcbpo,3400
-tutorials/snmd_test.py,sha256=-1CBLjcnfh4YvRE2m4K71DHMyCewAWLyJvg6rQK65Eo,4408
-tutorials/snmr_test.py,sha256=Iv8l_dReLhL6kqWuDTMbV43yjgO_8oiMWNEarCDnL3E,4610
-tutorials/snp_test.py,sha256=N5X1-Wdz5bG5gK4xmj1YMnzF_aKYR3ZYM7hGfpUPvjU,3471
-tutorials/ssp_test.py,sha256=d7HEwTJ2GjvQvJbqoI_UgTYr6WCaASV6yCxkzwcDSIw,4049
 tutorials/ssp_validation.py,sha256=AU7NlLrefiLVks5AI5AUU_lRkJBQL3-m70kj31FPp-s,8409
-tutorials/utility/__init__.py,sha256=wrFnRuC3FDjoipLYb46W8QrzGEu9h13F6xvjzdciNaY,342
-tutorials/utility/pipeline_examples/__init__.py,sha256=wrFnRuC3FDjoipLYb46W8QrzGEu9h13F6xvjzdciNaY,342
-tutorials/utility/pipeline_examples/ssp_pipeline.py,sha256=-nI2AZWm8PLiRVik4bHX0DFX11fWutVch7hl7nhtJX4,1408
-OmniGenome-0.0.1a0.dist-info/METADATA,sha256=OhgxGVimdPavyCky-wX9xOzjkp_pKcHKp_1uhJLheHY,835
-OmniGenome-0.0.1a0.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-OmniGenome-0.0.1a0.dist-info/top_level.txt,sha256=OoTd6f3-DXkxy5Faf24jXSk_UYjacEeSlFo0yqma9Bk,21
-OmniGenome-0.0.1a0.dist-info/RECORD,,
+OmniGenome-0.0.1b1.dist-info/METADATA,sha256=YYiFpBQ33HzmTOSJ3nUX50AA46vy5z5CWolrA5ybdg4,861
+OmniGenome-0.0.1b1.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
+OmniGenome-0.0.1b1.dist-info/top_level.txt,sha256=OoTd6f3-DXkxy5Faf24jXSk_UYjacEeSlFo0yqma9Bk,21
+OmniGenome-0.0.1b1.dist-info/RECORD,,
```

